{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Pruning Tutorial\n",
    "\n",
    "This is a tutorial specifically designed for the LotteryFL implementation. The methods and techniques listed below are used liberally throughout this repo. Below are all of the dependencies you need for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.utils.prune as prune\n",
    "from util import prune_fixed_amount, get_prune_params, train, create_model\n",
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch Model Primer\n",
    "\n",
    "This sections covers the structure of a PyTorch Module/Model. While TensorFlow has `tf.keras.Model` and `tf.keras.layers.Layer`, PyTorch only has `nn.Module`. This means both the model itself and its various layers are all sub-classes of `nn.Modules`. Therefore, both the model (e.g. VGG) and the layers (e.g. Conv2d) has `.named_parameters()` and `.named_buffers()` methods.\n",
    "\n",
    "When we say a PyTorch 'model', we are referring to an instance of `nn.Module` that may or may not have many sub-modules. Each of the sub-modules can also contain many sub-sub-modules. To demonstrate, below is a code snippet that allows you to access all of sub-modules in a PyTorch model recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (1): Linear(in_features=10, out_features=1, bias=True)\n",
      "),\n",
      "\n",
      "Linear(in_features=100, out_features=10, bias=True),\n",
      "\n",
      "Linear(in_features=10, out_features=1, bias=True),\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(100, 10), nn.Linear(10, 1))\n",
    "for module in model.modules():\n",
    "    print(module, end=',\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the modules returned by `model.modules()` include the model itself as well as the sub-modules. Below is a more complicated example where a `nn.Sequential` module is nested in another `nn.Sequential` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (1): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (2): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=3, bias=True)\n",
      "    (1): Linear(in_features=3, out_features=1, bias=True)\n",
      "  )\n",
      "),\n",
      "\n",
      "Linear(in_features=100, out_features=10, bias=True),\n",
      "\n",
      "Linear(in_features=10, out_features=10, bias=True),\n",
      "\n",
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=3, bias=True)\n",
      "  (1): Linear(in_features=3, out_features=1, bias=True)\n",
      "),\n",
      "\n",
      "Linear(in_features=10, out_features=3, bias=True),\n",
      "\n",
      "Linear(in_features=3, out_features=1, bias=True),\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(100, 10), \n",
    "                      nn.Linear(10, 10),\n",
    "                      nn.Sequential(\n",
    "                          nn.Linear(10, 3),\n",
    "                          nn.Linear(3, 1)\n",
    "                      )\n",
    "                     )\n",
    "for module in model.modules():\n",
    "    print(module, end=',\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to get a list of sub-modules to be pruned for a model, we need to check for whether the sub-modules returned by `model.modules()` are: 1. the model itself, 2. a `nn.Sequential` module. If the sub-modules is either 1. or 2., then we need to stop them from being pruned to avoid repetition. Below is the `get_prune_params()` method from `util.py`. We can see that the outer for-loop checks for whether a particular module satisfies either 1. or 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prune_params(model):\n",
    "    layers = []\n",
    "    \n",
    "    num_global_weights = 0\n",
    "    \n",
    "    modules = list(model.modules())\n",
    "    \n",
    "    for layer in modules:\n",
    "        \n",
    "        is_sequential = type(layer) == nn.Sequential\n",
    "        \n",
    "        is_itself = type(layer) == type(model) if len(modules) > 1 else False\n",
    "        \n",
    "        if (not is_sequential) and (not is_itself):\n",
    "            for name, param in layer.named_parameters():\n",
    "                \n",
    "                field_name = name.split('.')[-1]\n",
    "                \n",
    "                # This might break if someone does not adhere to the naming\n",
    "                # convention where weights of a module is stored in a field\n",
    "                # that has the word 'weight' in it\n",
    "                \n",
    "                if 'weight' in field_name and param.requires_grad:\n",
    "                    \n",
    "                    if field_name.endswith('_orig'):\n",
    "                        field_name = field_name[:-5]\n",
    "                    \n",
    "                    # Might remove the param.requires_grad condition in the future\n",
    "                    \n",
    "                    layers.append((layer, field_name))\n",
    "                \n",
    "                    num_global_weights += torch.numel(param)\n",
    "                    \n",
    "    return layers, num_global_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inner for loop goes through all of the parameters for each sub-modules. If the name of the parameter contains the word 'weight', then that means it is a weight instead of a bias. This might seem a 'hack' to distinguish which parameters are wieghts and which parameters are bias, but we have yet to figure out a better way to do it. In addition to checking whether a parameter is a weight, we also check if it is accounted for during the gradient calculation by check if `param.requires_grad` is True. Finally, the `get_prune_params()` method would a return a list of tuples. Each tuple is of the form `(nn.Module, str)` where the first entry is the reference of the module to be pruned, and the second entry is the name of the parameter to be pruned. We will now go to the second module where we can see how we can prune the model giving `get_prune_params()`.\n",
    "\n",
    "You might be wondering why we include the following code in the `get_prune_params()` method. We will get to that in the next section.\n",
    "\n",
    "``` python\n",
    "if field_name.endswith('_orig'):\n",
    "    field_name = field_name[:-5]\n",
    "```\n",
    "\n",
    "**Donglin's Note:** As I am writing this, I realized that it is easier to just go through the model (aka the 'root' module), call the `.named_parameters()`, and get all of the weights without the need of the inner for-loop. However, this does not allow us to see how much each 'layer' is getting pruned. I guess we can call this a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PyTorch Pruning Primer\n",
    "Let's start by defining a dummy MLP model with 2 layers of 10 and 2 neurons each (20 parameters total). Below, we can see the difference between `model.modules()` and `get_prune_params(model)` which we implemented above. As we can see below, the `get_prune_params(model)` only returns the modules that we need, whereas `model.modules()` returns a lot of 'wrapper' modules that don't need to be pruned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________________________________\n",
      "Returns from model.modules()\n",
      "=========================================================================================\n",
      "DummyMLP(\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Sequential(\n",
      "  (0): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n",
      "Linear(in_features=10, out_features=2, bias=True)\n",
      "_________________________________________________________________________________________\n",
      "Returns from get_prune_params(model)\n",
      "=========================================================================================\n",
      "(Linear(in_features=10, out_features=2, bias=True), 'weight')\n"
     ]
    }
   ],
   "source": [
    "class DummyMLP(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(DummyMLP, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(10, 2)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.classifier(x)\n",
    "model = DummyMLP()\n",
    "print('_________________________________________________________________________________________')\n",
    "print(\"Returns from model.modules()\")\n",
    "print('=========================================================================================')\n",
    "for layer in model.modules():\n",
    "    print(layer)\n",
    "    \n",
    "\n",
    "layers_to_prune, _ = get_prune_params(model)\n",
    "print('_________________________________________________________________________________________')\n",
    "print(\"Returns from get_prune_params(model)\")\n",
    "print('=========================================================================================')\n",
    "for layer in layers_to_prune:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have obtained that have a way to obtain the all of the layers to prune. However, before we do any pruning, let's first look at what gets returned by `model.named_parameters()` and `model.named_buffers()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________________________________\n",
      "Returns from model.named_parameters\n",
      "=========================================================================================\n",
      "[('classifier.0.weight', Parameter containing:\n",
      "tensor([[ 2.8054e-01,  2.2104e-04,  1.6230e-02,  1.1865e-01,  3.1461e-02,\n",
      "          1.3736e-01,  1.3134e-01, -2.8906e-01,  2.0455e-01, -4.2221e-02],\n",
      "        [ 2.4345e-01,  2.0637e-01, -2.5642e-01, -8.5109e-02, -6.6805e-02,\n",
      "         -1.5327e-01, -2.7775e-01, -1.9317e-01, -2.6005e-01,  2.7742e-01]],\n",
      "       requires_grad=True)), ('classifier.0.bias', Parameter containing:\n",
      "tensor([-0.2621,  0.0467], requires_grad=True))]\n",
      "_________________________________________________________________________________________\n",
      "Returns from model.named_buffers\n",
      "=========================================================================================\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print('_________________________________________________________________________________________')\n",
    "print(\"Returns from model.named_parameters\")\n",
    "print('=========================================================================================')\n",
    "print(list(model.named_parameters()))\n",
    "\n",
    "print('_________________________________________________________________________________________')\n",
    "print(\"Returns from model.named_buffers\")\n",
    "print('=========================================================================================')\n",
    "print(list(model.named_buffers()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the `.weight` attribute of a particular layer/module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 2.8054e-01,  2.2104e-04,  1.6230e-02,  1.1865e-01,  3.1461e-02,\n",
      "          1.3736e-01,  1.3134e-01, -2.8906e-01,  2.0455e-01, -4.2221e-02],\n",
      "        [ 2.4345e-01,  2.0637e-01, -2.5642e-01, -8.5109e-02, -6.6805e-02,\n",
      "         -1.5327e-01, -2.7775e-01, -1.9317e-01, -2.6005e-01,  2.7742e-01]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(layers_to_prune[0][0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do the pruning and see how the pruning will affect the inner structure of the model. We will use the `layers_to_prune` obtained from `get_prune_params()` and pass it into the `torch.nn.utils.prune.global_unstructured()` method. This method does the pruning in-place. We do not have to pass the entire model to `global_unstructured()`, only the `layers_to_prune`. This is because that `layers_to_prune` contains the references to the model layers. Therefore, any pruning can be done in-place using the references in `layers_to_prune` without the need to access the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('classifier.0.bias', Parameter containing:\n",
      "tensor([-0.2621,  0.0467], requires_grad=True)), ('classifier.0.weight_orig', Parameter containing:\n",
      "tensor([[ 2.8054e-01,  2.2104e-04,  1.6230e-02,  1.1865e-01,  3.1461e-02,\n",
      "          1.3736e-01,  1.3134e-01, -2.8906e-01,  2.0455e-01, -4.2221e-02],\n",
      "        [ 2.4345e-01,  2.0637e-01, -2.5642e-01, -8.5109e-02, -6.6805e-02,\n",
      "         -1.5327e-01, -2.7775e-01, -1.9317e-01, -2.6005e-01,  2.7742e-01]],\n",
      "       requires_grad=True))]\n",
      "[('classifier.0.weight_mask', tensor([[1., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
      "        [1., 1., 1., 0., 0., 0., 1., 1., 1., 1.]]))]\n"
     ]
    }
   ],
   "source": [
    "torch.nn.utils.prune.global_unstructured(layers_to_prune,\n",
    "                                         pruning_method=prune.L1Unstructured,\n",
    "                                         amount = 10)\n",
    "print(list(model.named_parameters()))\n",
    "print(list(model.named_buffers()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, the pruning module takes the original weights and add a '_orig' postfix behind it. The bias terms are left unchanged. If we access the `.weight` attribute in the first and only layer/module of the DummyMLP, we can see that the `requires_grad=True` entry is replaced with `grad_fn=<MulBackward0>`. This is done by the pruning module internally for gradient calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2805,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.2891,\n",
      "          0.2045, -0.0000],\n",
      "        [ 0.2435,  0.2064, -0.2564, -0.0000, -0.0000, -0.0000, -0.2777, -0.1932,\n",
      "         -0.2601,  0.2774]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(layers_to_prune[0][0].weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now covered all of the methods needed to construct the `prune_fixed_amount()` method in `util.py`. However, before we go through end the section, there is an importnat caveat we need to touch on. You might recall that we metioned the following snippet in the `get_prune_params()` method. \n",
    "\n",
    "``` python\n",
    "if field_name.endswith('_orig'):\n",
    "    field_name = field_name[:-5]\n",
    "```\n",
    "\n",
    "This is to check if a model has been pruned before. Below, we have defined a version of `get_prune_params()` without this code snippet. Given a DummyMLP with 20 weights, we first prune 5 weights and then 5 weights again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________________________________\n",
      "Model after 1st round of pruning\n",
      "=========================================================================================\n",
      "Pruning Summary\n",
      "Layers                                             Weight Name    Percent Pruned        Total Pruned\n",
      "-------------------------------------------------  -------------  ------------------  --------------\n",
      "Linear(in_features=10, out_features=2, bias=True)  weight         5 / 20 (25.00000%)               5\n",
      "Percent Pruned Globaly: 0.25\n",
      "\n",
      "[('classifier.0.bias', Parameter containing:\n",
      "tensor([-0.2877, -0.3014], requires_grad=True)), ('classifier.0.weight_orig', Parameter containing:\n",
      "tensor([[ 0.2245,  0.2599,  0.2035, -0.3123,  0.0654, -0.1012,  0.1646,  0.2186,\n",
      "         -0.2231, -0.1974],\n",
      "        [ 0.0487, -0.3001,  0.0796, -0.1016,  0.2889, -0.2548,  0.0478,  0.0560,\n",
      "          0.2037,  0.2126]], requires_grad=True))]\n",
      "_________________________________________________________________________________________\n",
      "Model after 2nd round of pruning\n",
      "=========================================================================================\n",
      "Pruning Summary\n",
      "Layers                                             Weight Name    Percent Pruned        Total Pruned\n",
      "-------------------------------------------------  -------------  ------------------  --------------\n",
      "Linear(in_features=10, out_features=2, bias=True)  weight_orig    5 / 20 (25.00000%)               5\n",
      "Percent Pruned Globaly: 0.25\n",
      "\n",
      "[('classifier.0.bias', Parameter containing:\n",
      "tensor([-0.2877, -0.3014], requires_grad=True)), ('classifier.0.weight_orig_orig', Parameter containing:\n",
      "tensor([[ 0.2245,  0.2599,  0.2035, -0.3123,  0.0654, -0.1012,  0.1646,  0.2186,\n",
      "         -0.2231, -0.1974],\n",
      "        [ 0.0487, -0.3001,  0.0796, -0.1016,  0.2889, -0.2548,  0.0478,  0.0560,\n",
      "          0.2037,  0.2126]], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "def get_prune_params(model):\n",
    "    layers = []\n",
    "    \n",
    "    num_global_weights = 0\n",
    "    \n",
    "    modules = list(model.modules())\n",
    "    \n",
    "    for layer in modules:\n",
    "        \n",
    "        is_sequential = type(layer) == nn.Sequential\n",
    "        \n",
    "        is_itself = type(layer) == type(model) if len(modules) > 1 else False\n",
    "        \n",
    "        if (not is_sequential) and (not is_itself):\n",
    "            for name, param in layer.named_parameters():\n",
    "                \n",
    "                field_name = name.split('.')[-1]\n",
    "                \n",
    "                # This might break if someone does not adhere to the naming\n",
    "                # convention where weights of a module is stored in a field\n",
    "                # that has the word 'weight' in it\n",
    "                \n",
    "                if 'weight' in field_name and param.requires_grad:\n",
    "                    \n",
    "                    # Might remove the param.requires_grad condition in the future\n",
    "                    \n",
    "                    layers.append((layer, field_name))\n",
    "                \n",
    "                    num_global_weights += torch.numel(param)\n",
    "                    \n",
    "    return layers, num_global_weights\n",
    "\n",
    "def prune_fixed_amount(model, amount, verbose=True):\n",
    "    parameters_to_prune, num_global_weights = get_prune_params(model)\n",
    "    prune.global_unstructured(\n",
    "        parameters_to_prune,\n",
    "        pruning_method=prune.L1Unstructured,\n",
    "        amount=amount)\n",
    "\n",
    "    num_global_zeros, num_layer_zeros, num_layer_weights = 0, 0, 0\n",
    "    global_prune_percent, layer_prune_percent = 0, 0\n",
    "    prune_stat = {'Layers': [],\n",
    "                  'Weight Name': [],\n",
    "                  'Percent Pruned': [],\n",
    "                  'Total Pruned': []}\n",
    "    \n",
    "    # Pruning is done in-place, thus parameters_to_prune is updated\n",
    "    for layer, weight_name in parameters_to_prune:\n",
    "        \n",
    "        num_layer_zeros = torch.sum(getattr(layer, weight_name) == 0.0).item()\n",
    "        num_global_zeros += num_layer_zeros\n",
    "        num_layer_weights = torch.numel(getattr(layer, weight_name))\n",
    "        layer_prune_percent = num_layer_zeros / num_layer_weights * 100\n",
    "        prune_stat['Layers'].append(layer.__str__())\n",
    "        prune_stat['Weight Name'].append(weight_name)\n",
    "        prune_stat['Percent Pruned'].append(f'{num_layer_zeros} / {num_layer_weights} ({layer_prune_percent:.5f}%)')\n",
    "        prune_stat['Total Pruned'].append(f'{num_layer_zeros}')\n",
    "        \n",
    "    global_prune_percent = num_global_zeros / num_global_weights\n",
    "    if verbose:\n",
    "        print('Pruning Summary', flush=True)\n",
    "        print(tabulate(prune_stat, headers='keys'), flush=True)\n",
    "        print(f'Percent Pruned Globaly: {global_prune_percent:.2f}', flush=True)\n",
    "\n",
    "model = DummyMLP()\n",
    "\n",
    "print('_________________________________________________________________________________________')\n",
    "print(\"Model after 1st round of pruning\")\n",
    "print('=========================================================================================')\n",
    "\n",
    "prune_fixed_amount(model, 5)\n",
    "print()\n",
    "print(list(model.named_parameters()))\n",
    "\n",
    "print('_________________________________________________________________________________________')\n",
    "print(\"Model after 2nd round of pruning\")\n",
    "print('=========================================================================================')\n",
    "prune_fixed_amount(model, 5)\n",
    "print()\n",
    "print(list(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see here, if we don't check for whether or not a weight has been pruned, it will treat 'weight_orig' as the actual name of the weight instead of just 'weight'. We can also see that, even though we have pruned 10 weights in total across the two rounds, the second pruning summary stil shows that we have only pruned 5 weights. This is because that, during the second round of pruning, if we use 'weight_orig' as the name of the parameter to be pruned, PyTorch pruning will think that this is a branch new set of weights with a completely different name. PyTorch's pruning module will not make the association between 'weight_orig' in the second round and the 'weight' in the first round. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Changing & Copying weights\n",
    "\n",
    "Other than the `get_prune_params` and `prune_fixed_amount` methods, we also have a lot of methods that takes care of copying models in `util.py`. As we have shown above, PyTorch's pruning methods stores the pruned weights in the `.weight` attribute of each layer. In addition, the pruning method will also add a '_orig' postfix behind the originial weight names. However, as illustrated below, changing the `.weight` field will have not effect on the final model output. Instead, we need to iterate through the named parameters of each layers and change them individually. The same techniques apply when copying the named buffers (masks) of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changing the \"weight\" directly will not work\n",
      "tensor([-0.1959, -0.2687], grad_fn=<AddBackward0>)\n",
      "Changing the named parameters WILL work\n",
      "tensor([0., 0.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = DummyMLP()\n",
    "\n",
    "layers, _ = get_prune_params(model)\n",
    "param_shape = layers[0][0].weight.shape\n",
    "prune_fixed_amount(model, 0, verbose=False)\n",
    "\n",
    "print('Changing the \"weight\" directly will not work')\n",
    "layers[0][0].weight = np.zeros(param_shape)\n",
    "output = model(torch.zeros(10,)) \n",
    "print(output)\n",
    "\n",
    "print('Changing the named parameters WILL work')\n",
    "for name, params in layers[0][0].named_parameters():\n",
    "    params.data = torch.zeros(params.shape)\n",
    "\n",
    "output = model(torch.zeros(10,)) \n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
