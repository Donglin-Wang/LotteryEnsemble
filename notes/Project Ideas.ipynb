{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How much shared learning is there?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lottery networks can be small, 1% of original size. How much shared learning will FederatedAverage provide is there is not much overlap in the weights of interest to various clients? First we need a sense of the influence clients are having on each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Influence Index (PII)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be good to have a measure of the influence client updates are having on each other when using sparse subnetworks at each client. We call this the parameter influence index, PII. We for some reasonable ways to define it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 (Server-side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* for each pair of clients, define\n",
    "  * $intersection(i, j) = | m_i \\odot m_j |$\n",
    "  * $intersectionRate(i, j) = \\frac{intersection(i, j)}{|\\theta|}$\n",
    "* for each client, define\n",
    "  * $pii_k = \\frac{1}{K-1} \\sum_{j \\, \\neq \\, k} intersectionRate(k, j)$\n",
    "* define\n",
    "  * $pii = \\frac{1}{K} \\sum_{k \\, = \\, 1}^{K} pii_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 (Client-side)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjust ClientUpdate$(C_k, \\theta_k^t, \\theta_0)$\n",
    "\n",
    "The idea is to compare $\\theta_k^t$ to $\\theta_k^{t-1}$, which is stored locally by clients.\n",
    "\n",
    "$updateCount(k) = |\\, \\mathbb{1} [ \\theta_k^t \\neq \\theta_k^{t-1} ] \\, |$ (possibly make an allowance for small floating point errors)\n",
    "\n",
    "$pii(k) = \\frac{updateCount}{|\\theta_k^t|}$\n",
    "\n",
    "Return $pii(k)$ to server from ClientUpdate$(C_k, \\theta_k^t, \\theta_0)$.\n",
    "\n",
    "Server averages these to compute pii."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modifications to LotteryFL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In LotteryFL, ClientUpdate$(C_k, \\theta_k^t, \\theta_0)$ prunes and then trains. This is not the LTH approach. It also implies that in early iterations the LTN is not being identified, and so the communication load remains high. At the beginning, the accuracy will be low, and so no pruning will occur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The client update could be changed to\n",
    "\n",
    "1. reverse the order to train then prune, or\n",
    "2. prune, train, and prune\n",
    "\n",
    "We do retain the existing conditions on pruning: accuracy check, and pruning target check."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2\n",
    "\n",
    "Summary: Single LTN, server-side pruning, client-side fine-tuning.\n",
    "\n",
    "Perhaps an LTN trained on one set of data will do OK if retrained for other data.\n",
    "\n",
    "* we start with plain FL\n",
    "* clientUpdate also returns local accuracy (a single number) to the server\n",
    "* server prunes when clients' accuracies are good enough (i.e. > acc_threshold)\n",
    "  this step has flexibility, we don't need ALL clients to meet this threshhold\n",
    "* we end up with a pruned model, computed by the server\n",
    "* repeat the above and we will get our LTN\n",
    "\n",
    "* for prediction, each client fine-tunes (by further-training, or training from scratch with initial weights) before making local predictions.\n",
    "\n",
    "Initial communication rounds will have high volume, but we have already commented that this would be the case with the LotteryFL algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Computing LTN with little data.\n",
    "\n",
    "The authors claim that they are addressing the use case of clients having as little as 5 images per class. That is not much data to go on when applying pruning to find the LT. With little data, does the LTH approach lead to incredibly sparse subnetworks, or to barely pruned networks? How well will such a subnetwork perform (generalize) on each client?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could perform LTH experiments on small data sets and report trends we observe. We could try to tie this back to LotteryFL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Communication Volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LotteryFL is supposed to lower the volume of communication between client server be leveraging a LT. But, how well does this work? How quickly (after how many rounds) does this pruning become significant.\n",
    "\n",
    "We can contrive and run a variety of experiments to determine if there is a relationship between volume of data on clients and rapidity of network pruning, or between the level of data skew and rapidity of network pruning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
