{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Federated Learning\n",
    "Communication-Efficient Learning of Deep Networks from Decentralized Data\n",
    "\n",
    "H. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, Blaise Ag√ºera y Arcas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "\n",
    "* explosion in the number of mobile devices\n",
    "* these have powerful processing capability\n",
    "* How can these be levereaged to train a global, shared model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications\n",
    "\n",
    "* language models\n",
    "  * speech recognition\n",
    "  * text entry\n",
    "* image classification\n",
    "  * likely to be shared or viewed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Considerations\n",
    "\n",
    "* Data privacy\n",
    "* Communicaton\n",
    "  * mobile network charge constrain level of communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Approach\n",
    "\n",
    "* split the training of a model across clients\n",
    "* aggregate the results of training on clients on a server\n",
    "* no training data is transfered from client to server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Learning vs Distributed Learning #1\n",
    "\n",
    "Federated learning involves:\n",
    "\n",
    "* Non-IID data\n",
    "* Unbalanced\n",
    "* Massively Distributed\n",
    "  * more clients than the number of samples per client\n",
    "* Limited communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Federated Learning vs Distributed Learning #2\n",
    "\n",
    "|                         | Computation Cost | Communication Cost |\n",
    "| :- | -: | :-: |\n",
    "| Federated Optimizaton   | Low | High |\n",
    "| Distributed Optimizaton | High | Low |\n",
    "\n",
    "Federated Learning seeks to  increased computation to decrease the number of rounds of communication require to achieve a well-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* the training set is the union of all (mutually disjoint) training samples from all clients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "* $n$: number of training samples\n",
    "* $K$: number of clients\n",
    "* $n_k$:number of samples for client k\n",
    "* $\\mathscr{P}_k$: training samples on client k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss\n",
    "\n",
    "The goal is to minimize the average of all the losses, $\\mathcal{f}_i(w)$, over the samples.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{f}(w) &= \\frac{1}{n} \\sum_{i=1}^n \\mathcal{f}_i(w)\\\\\n",
    "&= \\frac{1}{n} \\sum_{k=1}^K \\sum_{i=1}^{n_k} \\mathcal{f}_i(w)\\\\\n",
    "&= \\frac{1}{n} \\sum_{k=1}^K \\left( \\frac{n_k}{n_k} \\sum_{i=1}^{n_k} \\mathcal{f}_i(w) \\right) \\\\\n",
    "&= \\sum_{k=1}^K \\frac{n_k}{n} \\left( \\frac{1}{n_k} \\sum_{i=1}^{n_k} \\mathcal{f}_i(w) \\right) \\\\\n",
    "&= \\sum_{k=1}^K \\frac{n_k}{n} F_k(w)\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "This is the weighted average of the loss from each client."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining two trainings of a model\n",
    "\n",
    "The field of distributed optimization has come up with techniques for combining separately trained versions of a model. For two trainings, this can be done by taking a convex combination of the trained weights.\n",
    "\n",
    "$$\n",
    "w_{avg} = \\theta w_1 + (1 - \\theta) w_2, \\quad with \\, \\theta \\in [0, 1]\n",
    "$$\n",
    "\n",
    "For this to work, the models need to be trained in a compatible manner, by using the same random weight initialization. In the figure on the left below, the initial weights were not the same. We see that as the models are combined, the loss increases. On the other hand, the figure on the right shows the loss decreasing as the weights are combined.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](combining_models.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <tt>FederatedAverage</tt> Algorithm\n",
    "\n",
    "Training on a client can be seen as a large mini-batch SGD operation. In practice, the <tt>FederatedAverage</tt> algorithm trains on a fraction, $C \\in (0, 1)$, of the clients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](FedAvg_algorithm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.text_cell_render { width:75% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# change the cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.text_cell_render { width:75% !important; }</style>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
