{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lottery Ticket Hypothesis\n",
    "\n",
    "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks\n",
    "\n",
    "Jonathan Frankle, Michael Carbin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The Lottery Ticket Hypothesis:</b> A randomly-initialized, dense neural network contains a subnetwork\n",
    "that is initialized such that—when trained in isolation—it can match the test accuracy of the\n",
    "original network after training for at most the same number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may conjecture that:\n",
    "\n",
    "<div style=\"font-style:italic; margin-left: 2em\">\"SGD seeks out and trains a subset of well-initialized weights\"</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, Malach et al. prove that\n",
    "\n",
    "<div style=\"font-style:italic; margin-left: 2em\">\"A sufficiently over-parameterized neural network with random initialization contains a subnetwork that achieves competitive accuracy (with respect to the large trained network), without any training.\"</div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning\n",
    "\n",
    "\n",
    "* long history of pruning for neural networks (e.g. LeCun et al. 1990)\n",
    "* networks can be aggresively pruned (e.g. by 90%) without significant loss in accuracy\n",
    "* however, the pruned networks cannot be retrained effectively\n",
    "\n",
    "### Typical pruning strategy\n",
    "\n",
    "1. train network\n",
    "2. remove superfluous structure\n",
    "3. fine-tune the network\n",
    "4. optionally: repeat steps 2 and 3 iteratively\n",
    "\n",
    "Superflous\n",
    "* many possible notions of what is superfluous structure\n",
    "* for LTH, simply remove weights having the smallest magnitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's different about LTH?\n",
    "\n",
    "* Each unpruned connection's value is reset to its initialization from the original network <i>before</i> it was trained.\n",
    "\n",
    "To say it differently,\n",
    "<div style=\"font-style:italic; margin-left: 2em\">\"Weights pruned after training could have been pruned before training.\"</div>\n",
    "\n",
    "Previously Han et. al. observed\n",
    "\n",
    "<div style=\"font-style:italic; margin-left: 2em\">During retraining, it is better to retain the weights from the initial training phase for the connections that survived pruning than it is to re-initialize the pruned layers.\"</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The LTH approach\n",
    "\n",
    "## one-shot\n",
    "\n",
    "1. Randomly initialize a neural network $\\mathcal{f}(x; \\theta)$ (where $\\theta_0 \\sim \\mathscr{D}_\\theta)$.\n",
    "2. Train the network for $j$ iterations, arriving at parameters $\\theta_j$\n",
    "3. Prune $p\\%$ of the parameters in $\\theta_j$, creating a mask $m$, $m \\in \\{0, 1\\}^{|\\theta|}$.\n",
    "4. Reset the remaining parameters to their values in $\\theta_0$, creating the winning ticket $\\mathcal{f}(x; m \\odot \\theta)$\n",
    "\n",
    "## iterative pruning\n",
    "\n",
    "* the above steps are repeated for $n$ rounds\n",
    "* each round prunes $p^{\\frac{1}{n}}\\%$ of the weights that survived  the previous round"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caveats\n",
    "\n",
    "* subnetworks are found retroactively\n",
    "* finding subnetworks is very expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural information processing systems, pp. 598–605, 1990.\n",
    "\n",
    "[2] Eran Malach, Gilad Yehudai, Shai Shalev-Shwartz, Ohad Shamir. Proving the Lottery Ticket Hypothesis: Pruning is All You Need. <a href=\"https://arxiv.org/abs/arXiv:2002.00585\">arXiv:2002.00585</a>\n",
    "\n",
    "[3] Song Han, Jeff Pool, John Tran, William J. Dally. Learning bothWeights and Connections for Efficient Neural Networks. <a href=\"https://arxiv.org/abs/1506.02626\">arXiv:1506.02626</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.text_cell_render { width:75% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# change the cell width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.text_cell_render { width:75% !important; }</style>\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
